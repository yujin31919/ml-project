[
  {
    "objectID": "project(2).html",
    "href": "project(2).html",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "",
    "text": "seaborn에서는 붓꽃에 대한 데이터를 제공함\n붓꽃의 품종에 영향을 미치는 요인은 무엇인지 파악하고, 모델학습을 통해 품종을 예측하고자함"
  },
  {
    "objectID": "project(2).html#데이터-셋",
    "href": "project(2).html#데이터-셋",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "",
    "text": "seaborn에서는 붓꽃에 대한 데이터를 제공함\n붓꽃의 품종에 영향을 미치는 요인은 무엇인지 파악하고, 모델학습을 통해 품종을 예측하고자함"
  },
  {
    "objectID": "project(2).html#데이터-확인-및-전처리",
    "href": "project(2).html#데이터-확인-및-전처리",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "2. 데이터 확인 및 전처리",
    "text": "2. 데이터 확인 및 전처리\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# 데이터 불러오기\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# 데이터 확인\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\n\n# 데이터 전처리 : 결측값확인\ndf.isna().sum()\n\nsepal_length    0\nsepal_width     0\npetal_length    0\npetal_width     0\nspecies         0\ndtype: int64\n\n\n\n결측값은 없었음."
  },
  {
    "objectID": "project(2).html#데이터-분할-학습데이터-평가데이터",
    "href": "project(2).html#데이터-분할-학습데이터-평가데이터",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "3.데이터 분할 : 학습데이터, 평가데이터",
    "text": "3.데이터 분할 : 학습데이터, 평가데이터\n\n학습 데이터와 평가 데이터를 8:2 비율로 분할함\n\n\n# 학습/평가 데이터 분할(8:2 비율)\n\n\nX = df.drop(columns = 'species')     #feature\ny = df['species']                    #target\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
  },
  {
    "objectID": "project(2).html#데이터-탐색",
    "href": "project(2).html#데이터-탐색",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "4. 데이터 탐색",
    "text": "4. 데이터 탐색\n\n# 상자수염 그래프\n\ndf_train = pd.concat([X_train,y_train],axis = 1)\n\n\nplt.figure(figsize=(5,3))\nfor i,feature in enumerate(X_train.columns):\n    plt.subplot(2,2,i+1)         \n    sns.boxplot(x=feature,y='species',hue = 'species',data=df_train)\n    plt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 산점도\nsns.pairplot(df_train,hue='species',height =2)\nplt.show()"
  },
  {
    "objectID": "project(2).html#분류모델-학습",
    "href": "project(2).html#분류모델-학습",
    "title": "PROJECT2 : 붓꽃 분류",
    "section": "5. 분류모델 학습",
    "text": "5. 분류모델 학습\n\n# 최적의 k값 선택\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nk_range = range(1,20,2)\nk_scores = []\n\nfor k in range(1,2):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn,X_train,y_train, cv=5, scoring='accuracy')\n    k_scores.append(scores.mean())\n    print(f\"k={k}일때 정확도: {scores.mean():.3f}\")\n\nk=1일때 정확도: 0.950\n\n\n\n# 모델 성능이 가장 좋은 k 값 선택\nbest_k = k_range[k_scores.index(max(k_scores))]\nprint(f\"최적의 k값은 {best_k}이며 ,평균 정확도는{max(k_scores):.3f}\")\n\n최적의 k값은 1이며 ,평균 정확도는0.950\n\n\n\n# K-NN분류모델 생성 및 학습\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train,y_train)\n\n#학습데이터로 학습한 분류모델에 평가 데이터를 입력하여 클래스 분류\ny_pred = knn.predict(X_test)\n\n#분류모델 평가: 정확도, 정밀도, 재현도\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score\n\n\naccuracy = accuracy_score(y_test,y_pred)\nprecision = precision_score(y_test,y_pred,average='micro')\nrecall = recall_score(y_test,y_pred,average='micro')\n\n\nprint(f'정확도: {accuracy:.3f}')\nprint(f'정밀도: {precision:.3f}')\nprint(f'재현도: {recall:.3f}')\n\n정확도: 1.000\n정밀도: 1.000\n재현도: 1.000"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2025 창의융합과제연구",
    "section": "",
    "text": "python"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "project(1).html",
    "href": "project(1).html",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "",
    "text": "[데이터] scikit-learn에서 제공하는 1990년대 캘리포니아 주택 가격에 대한 데이터를 제공함\n캘리포니아 주택 가격에 영향을 미치는 요인은 무엇인지 파악하고,예측하는 회귀모델을 학습시키고자 함.\n총 20,640개의 관측값과 8개의 독립변수(feature), 1개의 종속변수(target)으로 구성됨"
  },
  {
    "objectID": "project(1).html#데이터-셋",
    "href": "project(1).html#데이터-셋",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "",
    "text": "[데이터] scikit-learn에서 제공하는 1990년대 캘리포니아 주택 가격에 대한 데이터를 제공함\n캘리포니아 주택 가격에 영향을 미치는 요인은 무엇인지 파악하고,예측하는 회귀모델을 학습시키고자 함.\n총 20,640개의 관측값과 8개의 독립변수(feature), 1개의 종속변수(target)으로 구성됨"
  },
  {
    "objectID": "project(1).html#데이터-확인-및-전처리",
    "href": "project(1).html#데이터-확인-및-전처리",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "2. 데이터 확인 및 전처리",
    "text": "2. 데이터 확인 및 전처리\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# 데이터 불러오기\n\nfrom sklearn.datasets import fetch_california_housing\ndata = fetch_california_housing(as_frame = True)\ndf = data.frame\ndf.head(3)\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n\n\n\n\n\n\n#데이터 확인\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   MedInc       20640 non-null  float64\n 1   HouseAge     20640 non-null  float64\n 2   AveRooms     20640 non-null  float64\n 3   AveBedrms    20640 non-null  float64\n 4   Population   20640 non-null  float64\n 5   AveOccup     20640 non-null  float64\n 6   Latitude     20640 non-null  float64\n 7   Longitude    20640 non-null  float64\n 8   MedHouseVal  20640 non-null  float64\ndtypes: float64(9)\nmemory usage: 1.4 MB\n\n\n\n캘리포니아 주택 가격 데이터를 확인한 결과, 관측값은 20,640 개, 변수는 9개로 나타남.\n주택 가격에 영향을 미치는 특성(feature)은 다음과 같음\n\nMedInc 지역 중위 소득 (10,000 USD)\nHouseAge 주택 연식 (건축 후 경과된 연도 수)\nAveRooms 가구당 평균 방 개수\nAveBedrms 가구당 평균 침실 개수\nPopulation 지역 내 인구 수\nAveOccup 가구당 평균 거주 인원 수\nLatitude 위도\nLongitude 경도\nMedHouseVal 주택 중위 가격 (100,000 USD)\n\n종속변수( target) 는 Medhouseval(주택중위가격)임\n\n\n\n# 데이터 전처리 : 결측값 확인\ndf.isna().sum()\n\nMedInc         0\nHouseAge       0\nAveRooms       0\nAveBedrms      0\nPopulation     0\nAveOccup       0\nLatitude       0\nLongitude      0\nMedHouseVal    0\ndtype: int64\n\n\n\n결측값은 없었음."
  },
  {
    "objectID": "project(1).html#데이터-분할-학습데이터-평가데이터",
    "href": "project(1).html#데이터-분할-학습데이터-평가데이터",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "3.데이터 분할 : 학습데이터, 평가데이터",
    "text": "3.데이터 분할 : 학습데이터, 평가데이터\n\n학습 데이터와 평가 데이터를 7:3 비율로 분할함\n\n\na,b = [10,20]\nprint(a)\nprint(b)\n\n10\n20\n\n\n\n# 학습/평가 데이터 분할(7:3비율)\nX = data.data      #feature\ny = data.target    #target\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=42)       \n# 0.3해서 비율나누고 난수(랜덤)로 시드값 형성하는거임..!\n\n\nX_train\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n7061\n4.1312\n35.0\n5.882353\n0.975490\n1218.0\n2.985294\n33.93\n-118.02\n\n\n14689\n2.8631\n20.0\n4.401210\n1.076613\n999.0\n2.014113\n32.79\n-117.09\n\n\n17323\n4.2026\n24.0\n5.617544\n0.989474\n731.0\n2.564912\n34.59\n-120.14\n\n\n10056\n3.1094\n14.0\n5.869565\n1.094203\n302.0\n2.188406\n39.26\n-121.00\n\n\n15750\n3.3068\n52.0\n4.801205\n1.066265\n1526.0\n2.298193\n37.77\n-122.45\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11284\n6.3700\n35.0\n6.129032\n0.926267\n658.0\n3.032258\n33.78\n-117.96\n\n\n11964\n3.0500\n33.0\n6.868597\n1.269488\n1753.0\n3.904232\n34.02\n-117.43\n\n\n5390\n2.9344\n36.0\n3.986717\n1.079696\n1756.0\n3.332068\n34.03\n-118.38\n\n\n860\n5.7192\n15.0\n6.395349\n1.067979\n1777.0\n3.178891\n37.58\n-121.96\n\n\n15795\n2.5755\n52.0\n3.402576\n1.058776\n2619.0\n2.108696\n37.77\n-122.42\n\n\n\n\n14448 rows × 8 columns\n\n\n\n\ny_test\n\n20046    0.47700\n3024     0.45800\n15663    5.00001\n20484    2.18600\n9814     2.78000\n          ...   \n17505    2.37500\n13512    0.67300\n10842    2.18400\n16559    1.19400\n5786     2.09800\nName: MedHouseVal, Length: 6192, dtype: float64\n\n\n\n20640*0.3            # 7:3으로 잘 분할됨..!\n\n6192.0"
  },
  {
    "objectID": "project(1).html#데이터-탐색",
    "href": "project(1).html#데이터-탐색",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "4. 데이터 탐색",
    "text": "4. 데이터 탐색\n\n# 요약통계량 확인\n\ndf_train = pd.concat([X_train,y_train],axis = 1)\ndf_train.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n7061\n4.1312\n35.0\n5.882353\n0.975490\n1218.0\n2.985294\n33.93\n-118.02\n1.93800\n\n\n14689\n2.8631\n20.0\n4.401210\n1.076613\n999.0\n2.014113\n32.79\n-117.09\n1.69700\n\n\n17323\n4.2026\n24.0\n5.617544\n0.989474\n731.0\n2.564912\n34.59\n-120.14\n2.59800\n\n\n10056\n3.1094\n14.0\n5.869565\n1.094203\n302.0\n2.188406\n39.26\n-121.00\n1.36100\n\n\n15750\n3.3068\n52.0\n4.801205\n1.066265\n1526.0\n2.298193\n37.77\n-122.45\n5.00001\n\n\n\n\n\n\n\n\n# 중위 주택 가격에 대한 분포\n\nplt.figure(figsize=(5,3))\nsns.histplot(df_train['MedHouseVal'],color = 'black',alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n# 상자그림으로 보자아\n\nplt.figure(figsize=(7,1))\nsns.boxplot(x ='MedHouseVal',data = df_train)\nplt.show()\n\n\n\n\n\n\n\n\n\n중위 주택 가격의 평균은 약 20만 달러로 나타남.(MedHouseVal)\n표준편차는 11만 달러로 나타남. 중위주택가격의 최소값은 1만 5천달러, 최대값은 50만달러로 나타남.\n히스토 그램과 상자그림을 살펴보면 중위주택 가격이 매우 높은 이상값이 많이 존재함을 알 수있음.\n\n\n#산점도\n\nsns.pairplot(df_train,height=0.8,plot_kws={'s':5},diag_kind='kde')\nplt.show()\n\n\n\n\n\n\n\n\n\n#상관계수\n\ncorr_train = df_train.corr()\ncorr_train\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\nMedInc\n1.000000\n-0.117506\n0.323255\n-0.071110\n0.003661\n0.024554\n-0.075892\n-0.019019\n0.688229\n\n\nHouseAge\n-0.117506\n1.000000\n-0.157529\n-0.087350\n-0.291589\n0.017437\n0.003461\n-0.101083\n0.106549\n\n\nAveRooms\n0.323255\n-0.157529\n1.000000\n0.845543\n-0.075529\n-0.004659\n0.111067\n-0.028503\n0.152106\n\n\nAveBedrms\n-0.071110\n-0.087350\n0.845543\n1.000000\n-0.071975\n-0.005809\n0.073138\n0.017361\n-0.048455\n\n\nPopulation\n0.003661\n-0.291589\n-0.075529\n-0.071975\n1.000000\n0.075019\n-0.101665\n0.092163\n-0.024316\n\n\nAveOccup\n0.024554\n0.017437\n-0.004659\n-0.005809\n0.075019\n1.000000\n0.007654\n-0.002295\n-0.020960\n\n\nLatitude\n-0.075892\n0.003461\n0.111067\n0.073138\n-0.101665\n0.007654\n1.000000\n-0.923408\n-0.141528\n\n\nLongitude\n-0.019019\n-0.101083\n-0.028503\n0.017361\n0.092163\n-0.002295\n-0.923408\n1.000000\n-0.049347\n\n\nMedHouseVal\n0.688229\n0.106549\n0.152106\n-0.048455\n-0.024316\n-0.020960\n-0.141528\n-0.049347\n1.000000\n\n\n\n\n\n\n\n\nupp_mat = np.triu(corr_train)\n\n\nplt.figure(figsize = (5,3))\nsns.heatmap(corr_train,annot=True,mask=upp_mat,vmin=-1,vmax=1)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n중위 주택 가격은 지역 중위 소득과 강한 양의 상관관계가 존재함\n또한 주택연식,가구당 평균 방 개수, 위도 간에도 약한 양의 상관관계가잇음\n따라서 중위 주택 가격을 예측하는 특성(feature)으로 해당 변수를 선택함."
  },
  {
    "objectID": "project(1).html#회귀모델-학습",
    "href": "project(1).html#회귀모델-학습",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "5. 회귀모델 학습",
    "text": "5. 회귀모델 학습\n\n# 상관관계가 존재하는 특성(feature) 선택\n# scikit-Learn에서 입력 데이터(feature)선택\nfeatures = ['MedInc','HouseAge','AveRooms','Latitude']\ntarget = 'MedHouseVal'\n\nX_train = df_train[features]                   # 2차원 배열\n\n\n# 선형 회귀모델 생성 및 학습\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train); # ; 찍으면 결과 안나옴.!\n\n\n\n#회귀계수(regression coefficient)\n\npd.DataFrame({'Features':features, 'Coefficient': model.coef_})\n\n\n\n\n\n\n\n\nFeatures\nCoefficient\n\n\n\n\n0\nMedInc\n0.435658\n\n\n1\nHouseAge\n0.016937\n\n\n2\nAveRooms\n-0.019505\n\n\n3\nLatitude\n-0.045071\n\n\n\n\n\n\n\n\n(1)회귀계수를 살펴보면 중위주택 가격은 지역 중위 소득이 1만 달러 증가하면 중위 주택 가격은 4만 3천 달러 증가하는 것으로 나타남.\n(2)또한 주택연식이 1년증가하면 중위주택가격은 1693달러 증가하는 것으로 나타남\n(3)중위주택 가격은 가구당 평균 방의 개수가 1개 증가하면 1950달러 감소하는 것으로 나타남\n(4)위도가 1도 증가하면 중위 주택 가격은 4507달러 감소하는 경향이있음\n\n(1)과 (4)에 의해 이는 경제적으로 여유가 있을수록 좋은 집에 거주함을 알 수 있으며, 일반적으로 캘리포니아주는 남부 지역이 상대적으로 대도시가 많아 주택 가격이 더 높은 것을 반영하고있다."
  },
  {
    "objectID": "project(1).html#회귀모델-평가",
    "href": "project(1).html#회귀모델-평가",
    "title": "Project(1) 캘리포니아 주택 가격 분석",
    "section": "6. 회귀모델 평가",
    "text": "6. 회귀모델 평가\n\n# 평가 데이터에서도 학습데이터에서 사용한 독립변수만 선택\nX_test = X_test[features]\n\n\n\n## 회귀모델 평가 : RMSE(MSE에 양의 제곱근), 결정계수\n#학습데이터로 학습한  회귀모델에 평가데이터를 입력해서 예측값 계산\ny_pred = model.predict(X_test)\n\n#평가 데이터의 실제 관측값과 예측값을 비교하여 모델을 평가\nfrom sklearn.metrics import mean_squared_error,r2_score\nRMSE = np.sqrt(mean_squared_error(y_test,y_pred))\nR2 = r2_score(y_test,y_pred)\n\nprint(f'RMSE :{RMSE:.3f}')\nprint(f'결정계수 :{R2:.3f}')\n\nRMSE :0.794\n결정계수 :0.520\n\n\n결과 및 시사점\n\nRMSE는 0.794로, 주택 중위 가격의 실제값과 예측값이 평균적으로 약 8만 달러 차이가 있다는 것을 의미함\n학습 데이터에서 주택 중위 가격의 평균은 약 20만 달러인 점을 고려하면, 모델의 평균 오차는 약 25% 수준임을 알 수 있음\n따라서 평균 오차가 비교적 큰 편이므로, 모델 성능 개선이 필요한 것으로 판단됨\n회귀계수를 살펴보면 지역 중위 소득, 주택 연식이 증가할수록 주택 중위 가격은 높아지며, 가구당 평균 거주 인원 수, 위도가 증가할수록 주택 중위 가격은 낮아지는 것으로 나타남"
  },
  {
    "objectID": "Titanic.html",
    "href": "Titanic.html",
    "title": "타이타닉 생존자 분석",
    "section": "",
    "text": "RMS 타이타닉은 영국의 화이트 스타 라인이 운영한 북대서양 횡단 여객선으로, 1912년 4월 10일 첫 출항하였다. 영국의 사우샘프턴을 떠나 미국의 뉴욕으로 향하던 중에 4월 15일 빙산과 충돌하여 침몰하였으며, 이로 인해 1,514명이 사망한 것으로 알려져 있다.\n타이타닉 데이터를 분석하여 생존에 영향을 준 요인을 파악하고, 이를 바탕으로 생존 여부를 예측하는 모델을 학습해보자."
  },
  {
    "objectID": "Titanic.html#데이터셋",
    "href": "Titanic.html#데이터셋",
    "title": "타이타닉 생존자 분석",
    "section": "1. 데이터셋",
    "text": "1. 데이터셋\n\n[데이터] seaborn에서 제공하는 1900년대 타이타닉 탑승자들에 대한 데이터를 제공함.\n타이타닉 생존여부에 영향을 미치는 요인은 무엇인지 파악하고,생존여부를 분류하는 모델을 학습시키고자 함.\n총 891개의 관측값과 15개의 독립변수(feature), 1개의 종속변수(target)으로 구성됨"
  },
  {
    "objectID": "Titanic.html#데이터-확인-및-전처리",
    "href": "Titanic.html#데이터-확인-및-전처리",
    "title": "타이타닉 생존자 분석",
    "section": "2. 데이터 확인 및 전처리",
    "text": "2. 데이터 확인 및 전처리\n\n# 라이브러리 불러오기\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# 데이터 불러오기\n\ndf = sns.load_dataset('titanic')\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf.info()    # 데이터 확인시 중복되는 변수나, 불필요한 변수는 제거할것임.\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\n타이타닉 데이터를 확인한 결과, 관측값은 891 개, 변수는 15개로 나타남.\n타이타닉호 생존에 영향을 미치는(feature)은 다음과 같음\n\npclass 객실등급(1,2,3)\nsex 성별\nage 나이\nsibsp 함께 탐승한 형제자매, 배우자 수\nparch 함께 탑승한 부모, 자식수\nfare 요금\nembarked 탑승항구(S,C,Q)\ndeck 갑판\nwho 사람구분(man,woman,child)\n\n종속변수( target) 는 ’survived’임\n\n\n# 데이터 전처리 :(1) 타이타닉 생존여부에 영향을 미치는 feature 선택\ndf1 = df[['survived','pclass','sex','age','sibsp','parch','fare','embarked','deck','who']].copy()\ndf1.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\ndeck\nwho\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nNaN\nman\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nC\nwoman\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nNaN\nwoman\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nC\nwoman\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nNaN\nman\n\n\n\n\n\n\n\n\n# 데이터 전처리: (2) 결측값 확인\ndf1.isna().sum()\n\nsurvived      0\npclass        0\nsex           0\nage         177\nsibsp         0\nparch         0\nfare          0\nembarked      2\ndeck        688\nwho           0\ndtype: int64\n\n\n\n# 'age' 결측값 처리 : 중앙값으로 대체.\n\ndf1['age'] = df1['age'].fillna(df1['age'].median())\n\n# 'deck' 결측값 처리 : 결측값이 너무 많아 제거.\n\ndf1.drop('deck', axis=1, inplace=True)\n\n# 'embarked' 결측값 처리: 최빈값으로 대체.\n\ndf1['embarked'] = df1['embarked'].fillna(df1['embarked'].mode()[0])\n\n\n\ndf1.isna().sum()   # 결측값 제거 확인\n\nsurvived    0\npclass      0\nsex         0\nage         0\nsibsp       0\nparch       0\nfare        0\nembarked    0\nwho         0\ndtype: int64"
  },
  {
    "objectID": "Titanic.html#데이터-분할-학습데이터-평가데이터",
    "href": "Titanic.html#데이터-분할-학습데이터-평가데이터",
    "title": "타이타닉 생존자 분석",
    "section": "3.데이터 분할 : 학습데이터, 평가데이터",
    "text": "3.데이터 분할 : 학습데이터, 평가데이터\n\n학습 데이터와 평가 데이터를 7:3 비율로 분할함\n타이타닉 데이터는 크기가 작기 때문에 7:3(train:test) 분할이 적절\n\n\n# 학습/평가 데이터 분할(7:3비율)\n# Feature, Target 분리\nX = df1.drop('survived', axis=1)      #feature\ny = df1['survived']                   #target\n\n# 학습/평가 데이터 분할 (7:3 비율\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nX_train\n\n\n\n\n\n\n\n\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nwho\n\n\n\n\n445\n1\nmale\n4.0\n0\n2\n81.8583\nS\nchild\n\n\n650\n3\nmale\n28.0\n0\n0\n7.8958\nS\nman\n\n\n172\n3\nfemale\n1.0\n1\n1\n11.1333\nS\nchild\n\n\n450\n2\nmale\n36.0\n1\n2\n27.7500\nS\nman\n\n\n314\n2\nmale\n43.0\n1\n1\n26.2500\nS\nman\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n3\nfemale\n21.0\n0\n0\n7.6500\nS\nwoman\n\n\n270\n1\nmale\n28.0\n0\n0\n31.0000\nS\nman\n\n\n860\n3\nmale\n41.0\n2\n0\n14.1083\nS\nman\n\n\n435\n1\nfemale\n14.0\n1\n2\n120.0000\nS\nchild\n\n\n102\n1\nmale\n21.0\n0\n1\n77.2875\nS\nman\n\n\n\n\n623 rows × 8 columns\n\n\n\n\ny_train\n\n445    1\n650    0\n172    1\n450    0\n314    0\n      ..\n106    1\n270    0\n860    0\n435    1\n102    0\nName: survived, Length: 623, dtype: int64"
  },
  {
    "objectID": "Titanic.html#데이터-탐색",
    "href": "Titanic.html#데이터-탐색",
    "title": "타이타닉 생존자 분석",
    "section": "4. 데이터 탐색",
    "text": "4. 데이터 탐색\n\n# 요약통계량 확인\n\ndf1_train = pd.concat([X_train,y_train],axis = 1)\ndf1_train.head()\n\n\n\n\n\n\n\n\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nwho\nsurvived\n\n\n\n\n445\n1\nmale\n4.0\n0\n2\n81.8583\nS\nchild\n1\n\n\n650\n3\nmale\n28.0\n0\n0\n7.8958\nS\nman\n0\n\n\n172\n3\nfemale\n1.0\n1\n1\n11.1333\nS\nchild\n1\n\n\n450\n2\nmale\n36.0\n1\n2\n27.7500\nS\nman\n0\n\n\n314\n2\nmale\n43.0\n1\n1\n26.2500\nS\nman\n0\n\n\n\n\n\n\n\n\n# 한글 깨짐 현상에 대한 해결 방법\nimport matplotlib as mpl\nplt.rc('font', family='Malgun Gothic')\nmpl.rcParams['axes.unicode_minus'] = False\n\n\ndf1['survived_label'] = df1['survived'].map({0: '사망', 1: '생존'})\n\n# 시각화할 변수\ncat_features = ['sex', 'pclass', 'embarked', 'who']\n\nplt.figure(figsize=(12,9))\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(2, 2, i + 1)\n    ax = sns.countplot(x=feature, hue='survived_label', data=df1)\n    plt.title(f'{feature}에 따른 생존자 수')\n    plt.ylabel('Count')\n\n\n\n\n\n\n\n\n\n# Figure 생성 (3행 1열)\nfig, axes = plt.subplots(3, 1, figsize=(6,9))\n\n# 1. 생존 여부에 따른 나이 분포 (boxplot)\nsns.boxplot(x='survived', y='age', data=df1, ax=axes[0])\naxes[0].set_title('생존 여부에 따른 나이 분포')\n\n# 2. 생존 여부에 따른 요금 분포 (boxplot)\nsns.boxplot(x='survived', y='fare', data=df1, ax=axes[1])\naxes[1].set_title('생존 여부에 따른 요금 분포')\n\n# 3. 나이 히스토그램 (생존 여부별)\nsns.histplot(data=df1, x='age', hue='survived', multiple='stack', ax=axes[2])\naxes[2].set_title('나이 분포 (생존 여부별)')\n\n# 레이아웃 정리\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n데이터 해석\n\n성별 (sex)에 따른 생존자 수는 여성 승객의 생존자 수가 남성보다 월등히 많았다. 특히 여성은 사망자보다 생존자가 많았지만, 남성은 사망자가 압도적으로 많았다. 이는 여성과 아이를 먼저 구조하는 원칙이 실제 구조 과정에 반영되었음을 보여준다.\n\n\n객실 등급 (pclass)에 따른 생존자 수는 1등급 승객의 생존률이 가장 높았고, 3등급 승객은 사망자가 더 많았다. 이는 상위 계층이 구조에 더 빠르게 접근할 수 있었거나, 위치상 구조가 용이했을 가능성을 시사한다.\n\n\n탑승 항구 (embarked)에 따른 생존자 수는 Cherbourg(C항)에서 탑승한 승객의 생존률이 높았으며, Southampton(S항) 탑승자는 수는 많았지만 사망률도 높았다. 항구별 승객의 계층 구성 혹은 배치 구조가 생존에 영향을 준 것으로 해석할 수 있다.\n\n\n사람 유형 (who)에 따른 생존자 수는 who 변수로 구분된 woman, child는 생존률이 매우 높았고, 반면 man은 생존률이 가장 낮았다. 이 역시 구조 우선 순위가 여성과 아동에게 있었음을 다시 한 번 보여준다.\n\n\n나이 (age)와 생존 여부 박스플롯과 히스토그램 분석 결과, 어린 승객일수록 생존 확률이 높은 경향을 보였으며, 노년층은 생존률이 낮았다. 구조 순서와 신체 조건 등 다양한 요인이 작용했을 것으로 판단된다.\n\n\n요금 (fare)과 생존 여부 생존자들은 평균적으로 더 높은 요금을 지불한 경향이 있었다. 요금은 객실 등급과 밀접한 관련이 있으므로, 상위 계층일수록 생존률이 높았음을 다시 확인할 수 있다."
  },
  {
    "objectID": "Titanic.html#분류-모델-학습",
    "href": "Titanic.html#분류-모델-학습",
    "title": "타이타닉 생존자 분석",
    "section": "5.분류 모델 학습",
    "text": "5.분류 모델 학습\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n\n# 0. 범주형 변수 숫자로 변환\ndf1['sex'] = df1['sex'].map({'female': 0, 'male': 1})\ndf1['embarked'] = df1['embarked'].map({'S': 0, 'C': 1, 'Q': 2})\ndf1['who'] = df1['who'].map({'child': 0, 'woman': 1, 'man': 2})\n\n\n# 1. Feature/Target 분리\nX = df1.drop(['survived', 'survived_label'], axis=1)  # survived 제외\ny = df1['survived']  # target은 0/1\n\n# 2. 학습/평가 데이터 분할 (7:3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n# 3. 최적의 k 찾기\nk_range = range(1, 20, 2)  # 1, 3, 5, ..., 19\nk_scores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n    k_scores.append(scores.mean())\n    print(f\"k = {k}일 때 평균 정확도: {scores.mean():.3f}\")\n\n\nk = 1일 때 평균 정확도: 0.668\nk = 3일 때 평균 정확도: 0.701\nk = 5일 때 평균 정확도: 0.701\nk = 7일 때 평균 정확도: 0.703\nk = 9일 때 평균 정확도: 0.711\nk = 11일 때 평균 정확도: 0.708\nk = 13일 때 평균 정확도: 0.713\nk = 15일 때 평균 정확도: 0.709\nk = 17일 때 평균 정확도: 0.714\nk = 19일 때 평균 정확도: 0.708\n\n\n\n# 4. 최적의 k 선택\nbest_k = k_range[k_scores.index(max(k_scores))]\nprint(f\"\\n✅ 최적의 k값은 {best_k}이며, 평균 정확도는 {max(k_scores):.3f}\")\n\n\n✅ 최적의 k값은 17이며, 평균 정확도는 0.714\n\n\n\n# 5. 최적의 k로 최종 모델 학습\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=17)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n17\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\n\n# 6. 예측 및 평가\ny_pred = knn.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n\nprint(f\"\\n📊 최종 평가 결과 (k={best_k})\")\nprint(f\"정확도: {accuracy:.3f}\")\nprint(f\"정밀도: {precision:.3f}\")\nprint(f\"재현도: {recall:.3f}\")\n\n\n📊 최종 평가 결과 (k=17)\n정확도: 0.713\n정밀도: 0.750\n재현도: 0.459"
  },
  {
    "objectID": "Titanic.html#분류-모델-평가",
    "href": "Titanic.html#분류-모델-평가",
    "title": "타이타닉 생존자 분석",
    "section": "6. 분류 모델 평가",
    "text": "6. 분류 모델 평가\n\n타이타닉 생존자 예측을 위해 K-NN 분류 모델을 적용하였으며, 교차 검증(cross-validation)을 통해 모델의 성능을 비교한 결과, k=17일 때 가장 높은 정확도를 기록하여 해당 값을 최적의 하이퍼파라미터로 선정하였다.\n최종적으로 학습된 K-NN 모델에 대해 테스트 데이터를 기반으로 성능을 평가한 결과는 다음과 같다:\n정확도는 0.713으로, 전체 테스트 샘플 중 약 71.3%를 올바르게 분류하였다. 이는 모델이 전체적으로는 비교적 안정적인 예측 성능을 갖추고 있음을 의미한다.\n정밀도는 0.750으로, 생존자(1)로 예측한 대상 중 실제로 생존자인 비율이 75%에 달한다. 즉, 모델이 생존자라고 판단한 예측은 상당히 신뢰할 수 있는 편이다.\n반면, 재현율은 0.459로 비교적 낮게 나타났다. 이는 실제 생존자 중 약 45.9%만을 모델이 생존자라고 제대로 예측했다는 의미로, 모델이 생존자를 놓치는 경우가 상당수 존재함을 시사한다. 종합적으로 볼 때, 해당 K-NN 모델은 생존자 예측에 있어서 정밀도는 높지만, 재현율이 낮은 보수적인 성향의 분류 모델로 해석할 수 있다. 이는 모델이 생존자임을 신중하게 판단하려는 경향이 있으며, 그로 인해 실제 생존자를 일부 놓치는 현상이 발생하는 것이다"
  }
]